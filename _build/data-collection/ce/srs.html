---
title: |-
  Simple Random Sampling
pagenum: 10
prev_page:
  url: /data-collection/ce/sampling-methods.html
next_page:
  url: 
suffix: .md
search: n frac x sample bar var sigma xi random sum e variance sampling y mean cov replacement population mu variables aligned left right simple srs without j lets covariance independent xj us p xy units same total value variable probability begin end mux muy where case not information approx small sqrt chance mathematically equal v s our m distinct values dots derive since hence sections covariances into important following measure between obtain rewrite independence correlate distribution terms given neq observations fraction accuracy standard deviation every subset being obtaining particular text choose binom supppose contains pm suppose pi ni occurrences represent

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">Simple Random Sampling</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In simple random sampling (SRS), every subset of $n$ units in the population has the same chance of being the sample.</p>
<p>Mathematically, if we are to sample $n$ units <strong>one at a time without replacement</strong> from a total population of $N$ units, the chance of us obtaining a particular sample is equal to:
$$\frac{1}{(N \text{ choose } n)} = \frac{1}{\binom{N}{n}}$$</p>
<h1 id="Sample-Mean-$\bar{X}_n$-v.s.-the-Population-Mean-$\mu$">Sample Mean $\bar{X}_n$ v.s. the Population Mean $\mu$<a class="anchor-link" href="#Sample-Mean-$\bar{X}_n$-v.s.-the-Population-Mean-$\mu$"> </a></h1><p>Supppose our population $P$ contains $m$ distinct values in total $p_1, p_2, \dots, p_m$. Suppose for each distinct value $p_i$, there is a total of $n_i$ occurrences for that value. Then, we can represent each sample element as a discrete random variable $X_i$ with the probability mass function:
$$P(X_i = p_j) = \frac{n_j}{N}, \:\: j=1,\dots,m$$
and we can derive its expectation and variance to be:
$$E[X_i] = \mu, \:\:\: Var[X_i] = \sigma^2$$
Since for simple random sampling, we know the sample mean $\bar{X}_n$ to be:
$$
\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i
$$
We have:
$$
\begin{aligned}
    E[\bar{X}_n] &amp;= E\left[\frac{1}{n}\sum_{i=1}^n X_i\right] \\
                 &amp;= \frac{1}{n} \sum_{i=1}^n E[X_i] \\
                 &amp;= \frac{1}{n} \sum_{i=1}^n \mu \\
                 &amp;= \mu
\end{aligned}
$$
Hence, we can see that with SRS, $E[\bar{X}_n] = \mu$. From previous sections, we conclude that <strong>$\bar{X}_n$ is an unbiased estimator of the true population mean $\mu$</strong>.</p>
<h1 id="Covariances">Covariances<a class="anchor-link" href="#Covariances"> </a></h1><p>Before jumping into the variance of the dataset obtained thorugh SRS, let's review the concept of covariance, which will be important in the following section.</p>
<p>Covariance ($Cov$) provides a measure of the <strong>strength of the (linear) correlation between two or more sets of random variables</strong>. Mathematically, it is defined as:
$$Cov(X, Y) = E[(X - \mu_X)(Y - \mu_Y)],$$
where $X$ and $Y$ are two random variables, and $\mu_X = E[X], \mu_Y = E[Y]$.</p>
<p>Similar to variance, we can further expand the inside of the expression and obtain an easier computational formula for covariance:
$$Cov(X, Y) = E[XY] - \mu_X \mu_Y$$</p>
<p>This also allows us to rewrite variance:</p>
<ul>
<li>$Cov(X, X) = Var[X]$.</li>
<li>$Var[X+Y] = Var[X] + Var[Y] + 2Cov(X, Y)$.</li>
</ul>
<h1 id="Independence">Independence<a class="anchor-link" href="#Independence"> </a></h1><p>Now that we know covariance a way to measure how strongly two random variables correlate with each other, let’s look into an extreme case where two random variables do not correlate with each other at all - independence.</p>
<p>Two random variables are independent if they <strong>convey no information about each other</strong>
and, as a consequence, receiving information about one of the two does not change our
assessment of the probability distribution of the other. In other words, the value of gaining information about two independent random variables with respect to each other is equal to 0. In more concrete terms, <strong>knowing about the outcomes and probability distribution of one random variable doesn’t help us find out about those of the other one if they are independent</strong>.</p>
<p>Given two independent random variables $X$ and $Y$, we know that:</p>
<ul>
<li>$E[X]E[Y] = E[XY]$</li>
<li>$Cov(X, Y) = 0$</li>
<li>$Var[X+Y] = Var[X] + Var[Y]$</li>
</ul>
<h1 id="Variance-of-the-Sample-Mean-$\bar{X}_n$">Variance of the Sample Mean $\bar{X}_n$<a class="anchor-link" href="#Variance-of-the-Sample-Mean-$\bar{X}_n$"> </a></h1><p>Let's first rewrite the variance in terms of covariances:
$$
\begin{aligned}
    Var[\bar{X}_n] &amp;= Var \left[ \frac{1}{n} \sum_{i=1}^n X_i \right] = \frac{1}{n^2} Var \left[\sum_{i=1}^n X_i \right] \\
    &amp;= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n Cov(X_i, X_j)
\end{aligned}
$$</p>
<p>To compute the variance, let's make use of another lemma:
if $i \neq j$, then the covariance between $X_i$ and $X_j$ is:
$$Cov(X_i, X_j) = -\frac{\sigma^2}{N - 1}$$</p>
<p>Now we are ready to derive the variance of the sample mean:
$$
\begin{aligned}
    Var[\bar{X}_n] &amp;= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n Cov(X_i, X_j) \\
    &amp;= \frac{1}{n^2}\sum_{i=1}^n Var(X_i) + \frac{1}{n^2} \sum_{i=1}^n \sum_{j\neq i} Cov(X_i, X_j) \\
    &amp;= \frac{\sigma^2}{n} - \frac{1}{n^2} n(n-1) \frac{\sigma^2}{N - 1} \\
    &amp;= \frac{\sigma^2}{n}\left(\frac{N-n}{N -1}\right) \\
    &amp;= \frac{\sigma^2}{n}\left(1 - \frac{n - 1}{N -1}\right)
\end{aligned}
$$</p>
<p>Hence, the variance of $\bar{X}_n$ is given by:
$$
Var[\bar{X}_n] = \frac{\sigma^2}{n}\left(1 - \frac{n - 1}{N -1}\right)
$$
Here're a few important observations:</p>
<ol>
<li>The factor $(1 - \frac{n-1}{N-1})$ is called <em>finite population correction</em>. We can see that $1 - \frac{n-1}{N-1} \approx 1 - \frac{n}{N}$, where the ratio $\frac{n}{N}$ is the <strong>sampling fraction</strong>.</li>
<li>We can see that $(1 - \frac{n-1}{N-1})$ is always less than one. Therefore, we have: $$Var[\bar{X}_n] &lt; \frac{\sigma^2}{n}$$. We will explore in later sections on what this inequality implies.</li>
<li>If the sampling fraction is small ($n &lt;&lt; N$), then: $$Var[\bar{X}_n] \approx \frac{\sigma^2}{n}, \:\:\: \sigma_{\bar{X}_n} \approx \frac{\sigma}{\sqrt{n}}$$</li>
<li>To double the accuracy of the approximation $\bar{X}_n$, the sample size $n$ must be quadrupled.</li>
<li>If $\sigma$ is small (population values are not very dispersed), then a small sample will be fairly accurate. However, as $\sigma$ increases, we will need a larger sample to obtain the same accuracy.</li>
</ol>
<h1 id="Sampling-with-Replacement">Sampling with Replacement<a class="anchor-link" href="#Sampling-with-Replacement"> </a></h1><p>Let's look at a simpler case of simple random sampling, done with replacement. In this case,<strong>each sample random variable $X_i$ can be treated as independent of each other</strong>. This would give us the following expressions for the sample mean, sample variance, and sample standard deviation.</p>
<ul>
<li><strong>Sample Mean</strong>: $$E[\bar{X}_n] = \mu$$</li>
<li><strong>Sample Variance</strong>: $$Var[\bar{X}_n] = \frac{1}{n^2} \sum_{i=1}^n Var[X_i] = \frac{1}{n^2} \sum_{i=1}^n \sigma^2 = \frac{\sigma^2}{n}$$</li>
<li><strong>Sample Standard Deviation</strong>: $$\sigma_{\bar{X}_n} = \sqrt{Var[\bar{X}_n]} = \frac{\sigma}{\sqrt{n}}$$</li>
</ul>
<h2 id="SRS-(Without-Replacement)-v.s.-Sampling-With-Replacement">SRS (Without Replacement) v.s. Sampling With Replacement<a class="anchor-link" href="#SRS-(Without-Replacement)-v.s.-Sampling-With-Replacement"> </a></h2><p>Now, back to the second and third observations we have made about SRS without replacement above:</p>
<ol>
<li>Since we know that $Var[\bar{X}_n] &lt; \frac{\sigma^2}{n}$, this means simple random sampling (without sampling) is <strong>more efficient than sampling with replacement</strong>.</li>
<li>When $n &lt;&lt; N$, we can see that the sample mean and the variance of the sample mean is <strong>approximately the same as if we sample with replacement</strong>.</li>
</ol>
<h2 id="One-Last-Thing-to-Note">One Last Thing to Note<a class="anchor-link" href="#One-Last-Thing-to-Note"> </a></h2><p>In Simple Random Sampling, we sample <strong>without replacement</strong>.</p>

</div>
</div>
</div>
</div>

 


    </main>
    