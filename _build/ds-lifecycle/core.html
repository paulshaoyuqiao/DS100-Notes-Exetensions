---
title: |-
  At the Core of the Data Science Lifecycle...
pagenum: 2
prev_page:
  url: /ds-lifecycle/ds-lifecycle.html
next_page:
  url: /ds-lifecycle/pitfalls.html
suffix: .md
search: data our methods model dataset us usually proper science step lifecycle question process models field insights perspective problem acquiring cleaning analysis via portion technique allows cross follow series scientific gain into area repeatedly formulate need forming also background within once build quite statistical sampling various ways such population different exploratory eda used numerical visually discover form better experiments conclusions lastly features train performance highly disciplinary requiring dual lens computational inferential thinking scientist analyst generally allow increasingly studying applied last iteration helps motivate next primarily consists following steps formulating properly not only hypothesis experiment drawing observation utilize expertise researching having contextual

comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---

    <main class="jupyter-page">
    <div id="page-info"><div id="page-title">At the Core of the Data Science Lifecycle...</div>
</div>
    <div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Data science is a highly cross-disciplinary field, requiring the dual lens of computational and inferential thinking. As a data scientist/analyst, we generally follow a series of scientific methods to allow us to increasingly gain insights into the area we are studying (from the data perspective). This series of methods can be applied repeatedly, and the last step of each iteration usually helps motivate the next one. The <strong>data science lifecycle</strong> primarily consists of the following 4 steps:</p>
<ol>
<li>Formulating a question or problem<ul>
<li>To properly formulate a question, we usually not only need to follow the scientific process of forming a hypothesis/experiment or drawing an observation, but also utilize <strong>area expertise</strong> for the field we are researching on. <strong>Having a proper contextual background</strong> (usually that means doing background research in advance or collaborate with professionals within the field) is crucial to forming a question with minimal bias.</li>
</ul>
</li>
<li>Acquiring and cleaning data<ul>
<li>Once the problem is formed, we will need data, the core block of evidence, to build our analysis and support our claim.</li>
<li>The data acquisition process could be quite diverse. In this class we will explore this stage from mainly 2 perspectives: <strong>the statistical perspective via sampling methods</strong> and <strong>the data perspective via various data cleaning and restructing techniques</strong>. We will also look at ways to take advantage of the modern file system and the Internet to collect data quickly and effectively via the use of <strong>web technologies</strong>.</li>
<li>Quite contrary to popular belief, the major portion of the data science lifecycle is actually devoted to <strong>acquiring and cleaning the data</strong> such that it is proper and sufficiently "unbiased" to derive a model upon. Practically speaking, since the cost of acquiring the data such that it comprehensively covers the entire studied population is so high that it rarely ever happens, we will have to make do our ways with different <strong>sampling and estimation methods</strong>. The choice of which of these approximation methods to use crucially influence how our data-dependent models will perform and reflect about the problem we are analyzing.</li>
</ul>
</li>
<li>Conducting exploratory data analysis (EDA)<ul>
<li>As we have seen from the previous step, the methods we have used to further understand and restructure the data are largely <strong>numerical and quantitative</strong>. </li>
<li><strong>Exploratory Data Analysis</strong> is another great statistical technique that allows us to summarize the main characteristics visually. For a lot of us, seeing things visually allows us to intuitively discover various trends and patterns within the dataset. </li>
<li>EDA, as a <strong>visual technique</strong>, is in no way as rigorous as the <strong>quantitiave methods</strong> in the data collection (step 2) or the data modeling (step 4) process. However, the free-form nature of the technique allows us to reformulate better hypotheses and experiments that may better resemble the population / answer our original question. </li>
</ul>
</li>
<li>Using prediction and inference to draw conclusions<ul>
<li>Lastly, once we have a deeper understanding of the dataset and have prepared it in a proper form, we can choose from a variety of <strong>statistcal and numerical methods</strong> to build models that help predict and infer insights from it.</li>
<li>A proper model is centered around the use of <strong>proper summary statistics</strong>. From there, we <strong>engineer features</strong> and <strong>train our model on a portion of the dataset</strong>. Usually, in order to discover and isolate the features that impact the model's performance the most, we will have to <strong>repeatedly train and assess our model on different segments of the dataset</strong>, a process called cross-validation. Lastly, we run our model on the remaining portion of the dataset, <strong>the testing dataset</strong>, and analyze the accuracy of our model. </li>
<li>Depending on what task our model is being used for and its relative performance, we can usually either formulate new experiments (and restart our data science lifecycle wohooo!) to gain more refined insights or incoporate the models/conclusions from the lifecycle into other tasks. </li>
</ul>
</li>
</ol>

</div>
</div>
</div>
</div>

 


    </main>
    